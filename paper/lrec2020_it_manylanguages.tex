\documentclass[10pt, a4paper]{article}

\usepackage{lrec}
\usepackage{multibib}
\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}

\usepackage{booktabs}
\usepackage[symbol]{footmisc}

% for eps graphics
% 
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

%\usepackage{hyperref}
\usepackage{xstring}

\usepackage{color}

\definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}


\title{Exploiting Cross-Lingual Hints to Discover Event Pronouns}

\name{Sharid Loáiciga$^{1*}$, Christian Hardmeier$^2$  \& Asad Sayeed$^3$}

\address{$^1$ CoLab Potsdam, Department of Linguistics, University of Potsdam \\  
$^2$ Department of Linguistics and Philology, Uppsala University \\
$^3$ CLASP, Department of Philosophy, Linguistics and Theory of Science, University of
Gothenburg  \\ 
sharid.loaiciga@gmail.com, christian.hardmeier@lingfil.uu.se, asad.sayeed@gu.se}


\abstract{ Non-nominal co-reference is much less studied than nominal
coreference, partly because of the lack of annotated corpora. We explore the 
possibility of exploiting parallel multilingual corpora as a means of cheap 
supervision for the classification of three different readings of the English 
pronoun \textit{it}: entity, event or pleonastic, from their translation in 
several languages. We found that the `event' reading is not very frequent, but can be easily predicted provided that the construction used to translate the \textit{it} example is a pronoun as well. These cases, nevertheless, are not enough to generalize to other types of non-nominal reference. \\ \newline
\Keywords{\textit{it}, reference, Europarl corpus} }


\begin{document}


\maketitleabstract

\section{Introduction}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\footnotetext[1]{Work completed while the first author was affiliated with the
Department of Philosophy, Linguistics and Theory of Science, University of Gothenburg.}

Nominal coreference has been studied extensively, but work on the automatic 
recognition of non-nominal anaphora is scarce, as are annotated data sets. Among 
the challenges of non-nominal anaphora is the difficulty of characterizing the 
large variance of antecedent types, which often include clauses, sentences, and 
even paragraphs. Here we focus on the English pronoun \textit{it} and its 
capacity to function as anaphor for nominal entity and non-nominal event 
antecedents, and as a pleonastic token. Examples \ref{ex:nominal} to 
\ref{ex:pleo} below illustrate these different readings using English passages 
from the Europarl corpus and their French parallel translations.

%Depending on the context, the English pronoun \textit{it} can express anaphoric 
%reference with a nominal entity antecedent or with a
%non-nominal antecedent such as an event. It can also be used
%pleonastically.  

In this paper, we evaluate the potential of multilingual parallel data as a 
source of indirect supervision for the annotation and classification of 
different readings of the English 
pronoun \textit{it}. We explore the hypothesis that languages have different 
strategies and preferences to encode referential relationships, and that these 
differences surface as systematic patterns in multilingual parallel data. 
Therefore, the competing readings of the pronoun \textit{it} correspond to different patterns of translation across languages.  

We present a method for creating artificially labeled data for the 
classification of three different readings of \textit{it}: entity, event or 
pleonastic, from their translation in several languages. We found that the 
`event' reading is not very frequent, but can be easily predicted provided that 
the construction used to translate the \textit{it} example is a pronoun as well. These 
cases, nevertheless, are not enough to generalize to other types of non-nominal 
reference. %Deictic uses in particular, are expressed very differently and are 
%therefore difficult to normalize.

%Anaphoric non-nominals are typically discarded in the non-referring basket
%together with pleonastics. 
%

\begin{enumerate}

\item\label{ex:nominal} \textsc{Entity reading}

Madam President, I have been deluged with messages from growers from all over 
the south-east of England who regard this proposal as near catastrophic. 
\textbf{It} will result, they tell me, in smaller crops and in higher prices. 

\textit{Madame la Présidente, j'ai été assailli de messages de cultivateurs 
en provenance de tout le sud-est de l'Angleterre, qui considèrent cette 
proposition comme une quasi-catastrophe. \textbf{Elle} entraînera, me 
disent-ils, une baisse de les rendements agricoles et une augmentation des 
prix.}

%The infectious disease that's killed more humans than any other is
%\textbf{malaria}.  \textbf{It}'s carried in the bites of infected mosquitos.

%\textit{Jene Krankheit, die mehr Leute als jede andere umgebracht hat, ist
%Malaria gewesen. \textbf{Sie} wird über die Stiche von infizierten Moskitos
%übertragen.}

\item\label{ex:event}\textsc{Event reading}

The European Parliament has always taken a vigorous stance against racism and
ethnic intolerance. I appeal to you, as Members of this House, to do
\textbf{it} once again and support our written declaration condemning Turkish racism against Bulgarians.

\textit{Le Parlement européen a toujours pris des positions véhémentes contre le racisme
et l'intolérance ethnique. Je fais appel à vous, en tant que membres de cette
Assemblée, pour que vous \textbf{le} fassiez à nouveau, et que vous souteniez notre
déclaration écrite condamnant le racisme turc à l'égard des Bulgares.} 

%But I think \textbf{if we lost everyone with Down syndrome},  \textbf{it} would
%be a catastrophic loss.

%\textit{Aber, wenn wir alle Menschen mit Down-Syndrom verlören, wäre
%\textbf{das}    ein katastrophaler Verlust.}

\item\label{ex:pleo} \textsc{Pleonastic reading}

Since the beginning of October 2008 I have been trying to get speaking time in
the one-minute contributions and I am pleased that I have finally succeeded.
\textbf{It} is interesting that Mr Rogalski has been allowed to speak three times in the meantime. 


\textit{Depuis le début d'octobre 2008, j'ai essayé d'obtenir un temps de parole
dans le cadre des interventions d'une minute et je suis heureux d'avoir
finalement réussi. \textbf{Il} est intéressant que M. Rogalski ait été autorisé à prendre la parole trois fois dans l'intervalle.}



%And  \textbf{it} seemed to me that there were three levels of acceptance that
%needed to take place.

%\textit{Und \textbf{es} schien, dass es drei Stufen der Akzeptanz gibt, die alle
%zum Tragen kommen mussten.}

\end{enumerate}



\section{Related Work}

Reference to non-nominal antecedents has largely been a niche area in NLP
research. It is extensively surveyed in detail in a recent article by
\newcite{kolhatkar-etal-2018-survey}. The most extensive annotation efforts in
the field of coreference resolution have focused on nominal coreference.
OntoNotes \cite{Pradhan:2013}, the largest and most frequently used corpus for
training coreference resolution systems, for instance, only includes verbs if
``they can be co-referenced with an existing noun phrase'' according to its
guidelines. Corpora with a richer annotation of event pronouns exist, but are
much smaller. The most important resource is the ARRAU corpus
\cite{poesio-etal-2018-anaphora}, whose size amounts to about 20\% of version~5
of OntoNotes. ParCorFull \cite{Lapshinova-Koltunski-Hardmeier-Krielke2018} also
contains annotations of event pronouns.

The scarcity of manually annotated resources has led to the use of artificial
training data for the resolution of non-nominal anaphora.
\newcite{kolhatkar-etal-2013-interpreting} study the resolution of anaphoric
shell nouns such as `this issue' or `this fact' by exploiting
cataphoric instances such as `the fact that\ldots'.
\newcite{marasovic-etal-2017-mention} construct training examples based on
specific patterns of verbs governing embedded sentences. As far as we know, the
use of multilingual data for automatic data creation is novel in our work.


%They give the following example:
%
% Sales of passenger cars [grew]x 22\%. [The strong growth]x followed
% year-to-year increases.
% 
Before the breakthrough of neural end-to-end systems in coreference resolution
\cite{Lee:2017}, coreference resolvers needed to do explicit mention
classification in order to exclude non-referential mentions before any
resolution was attempted. In this context, the pronoun \textit{it} has been targeted,
as many of its uses are non-referential. \newcite{Evans2001} proposes the
classification of the pronoun \textit{it} into seven classes using contextual features.
\newcite{Boyd2005} report similar results of around 80\% accuracy using more
complex syntactic patterns. \newcite{Bergsma:2011} describe a system for
identifying non-referential pronouns using web $n$-gram features, however
without accounting explicitly for event reference.

The many uses of \textit{it} are also particularly relevant in dialog texts, where
event reference is much more common than in news data. In this context,
\newcite{muller-2007-resolving} proposes a disambiguation of \textit{it} together with
the deictic pronouns \textit{this} and \textit{that}.  Finally, \newcite{Lee2016} create a corpus
for \textit{it}-disambiguation in question answering, a domain close to dialog. It is
worth noting that current coreference resolution systems are not trained to
manage dialog data.

More recently,  \newcite{Loaiciga2017itrnn} proposed a semi-supervised setup
based on a combination of syntactic and semantic features. They used these features in a two-step
classification approach where a maximum entropy classifier was applied first and a
recurrent recursive network (RNN) after. \newcite{yaneva-etal-2018-classifying},
on the other hand, report on experiments using features from eye gaze that prove
to be more effective than any of the other types of features reported in
previous works.




\section{Method}

We work with the corpus Europarl \cite{Koehn2005} v8 as provided in the OPUS 
collection \cite{TIEDEMANN12.463}. OPUS includes parsed, sentence-level 
and word-level alignments files, as well as a toolbox for corpus processing 
\cite{aulamo-et-al-opus}. 

We use all 15 languages paired with English as the 
source language. The languages are German, Spanish, Estonian, Finnish, French, 
Hungarian, Italian, Latvian, Dutch, Polish, Portuguese, Romanian, Slovak, 
Slovenian, and Swedish. The data labeling method is as follows:

\begin{enumerate}\setlength\itemsep{1em}

\item Europarl is a parallel corpus of translations
between the language pairs, but the amount of data from one language to another
varies. Therefore, we begin by extracting only the set of common sentences
across all languages. This already reduces the data from 2,039,537 segments to
281,346. 

\item  Next, we rely on the English parsed files to
identify all instances of the pronoun \textit{it}.

\item We then use the word-level alignment files to
extract the aligned translation in each of the target languages.

Word alignment is not perfect. One-to-one correspondences are unstable for
particles and other small word forms, particularly if they depend on verbs and 
might be translated by just one verb form, thus virtually disappearing from the
translation. Pronouns in particular, depending on the language, might not be
translated if, e.g.,~the language allows pro-drop, or they might be
translated with a construction that is not a pronoun, e.g.,~if there is a 
mismatch in the number of arguments between the source and target verbs.

%she was awarded the medal -> on lui a donné une medaille 

For improving the quality of the word alignments, we use a window of -3 and
+3 tokens before and after the position of the aligned token. This means that if
the translated token is not a pronoun (we have POS information from the parsing
files), we search for a pronoun translation within the window range.

\item To label the English instances of \textit{it} as `entity', `event' or 
`pleonastic' we use French as a seed language.

We consider all instances 
translated with the neutral demonstrative pronouns \textit{cela}, \textit{ceci} or 
\textit{ça} as events. In French, these pronouns are typically used to refer to 
proposition or phrases. 

For the entity nominal case, we took the French translations \textit{elle} and 
\textit{il}.

Last, for the `pleonastic' readings, we took all instances of \textit{it} analyzed 
as expletives in the parsed files. These files have been processed using universal 
dependencies v2.0 (UDPipe parser, models from 2017-08-01), which includes the 
dedicated dependency relation \texttt{expl} \cite{bouma-etal-2018-expletives}.

From 69,126 \textit{it} pronouns, we label 22,615 instances, corresponding to 
approximately  30\% (Table \ref{tab:labelingtotals}).

\begin{table}[h]\centering 
\begin{tabular}{cccc} 
\toprule \textbf{English} &
\textbf{French} &  \textbf{Class} & \textbf{Instances}\\ 
\midrule 
it &  \textit{elle/il}  & entity & 11,483\\
it & \textit{cela/ça/ceci} & event &  910\\ 
it  &  -- &pleonastic & 10,222 \\
\bottomrule
\end{tabular} 
\caption{Summary of the translation assumptions and the total number of examples 
annotated automatically. }\label{tab:labelingtotals} 
\end{table}


\item The translations from the other 14 languages that are not French are used as 
features in a classification task (Section \ref{sec:experiments}). We present an example in 
Figure \ref{fig:featuresexample}, where each line represents a feature vector.

\end{enumerate}


%\begin{table}[h!]\centering 
%\begin{tabular}{ccc}
%\multicolumn{3}{c}{\textbf{Label}}\\ 
%\toprule 
%Entity & Event & Pleonastic \\
%\midrule 
%11,483 & 910 &10,222\\ 
%\bottomrule 
%\end{tabular} 
%\caption{Resulting distribution after automatic labeling. }
%\label{tab:resultinglabels} 
%\end{table}



\begin{center} \begin{figure*} \resizebox{\linewidth}{!}{
\begin{tabular}{*{14}{l}}
\toprule
%\textbf{English} & \textbf{French} &  \textbf{Class} &&&&&&&&&&& \\ \midrule it
%&  \textit{elle/il}  & entity  &&&&&&&&&&& \\ it & \textit{cela/ça/ceci} &
%event  &&&&&&&&&&& \\ it  &  --  &&&&&&&&&&& \\ \midrule
\multicolumn{14}{c}{\textbf{Features}}\\ \midrule DE & ES & ET & FI &HU& IT &
LV&NL & PL & PT &  RO& SK& SL& SV  \\ &&&&&&&&&&&&&\\ 

\textit{empty}&\textit{idea}&\textit{seda}&\textit{empty}&\textit{képeznie}&\textit{essenza}&\textit{es}&\textit{dit}&\textit{dodać}&\textit{adaug}&\textit{že}&\textit{empty}&\textit{empty}&\textit{detta}\\

\textit{du}&\textit{usted}&\textit{sa}&\textit{empty}&\textit{te}&\textit{l'}&\textit{empty}&\textit{u}&\textit{empty}&\textit{empty}&\textit{eşti}&\textit{ty}&\textit{empty}&\textit{du}\\

\textit{empty}&\textit{señor}&\textit{ja}&\textit{empty}&\textit{.}&\textit{-}&\textit{empty}&\textit{ik}&\textit{cohn-bendit}&\textit{cohn-bendit}&\textit{îi}&\textit{a}&\textit{gospod}&\textit{sluta}\\

\textit{empty}&\textit{que}&\textit{juhataja}&\textit{siirtämisestä}&\textit{úr}&\textit{presidente}&\textit{empty}&\textit{de}&\textit{!}&\textit{é}&\textit{,}&\textit{je}&\textit{predsednik}&\textit{det}\\

\textit{empty}&\textit{es}&\textit{üksluine}&\textit{ne}&\textit{dolog}&\textit{in}&\textit{tas}&\textit{empty}&\textit{co}&\textit{empty}&\textit{ce}&\textit{spôsobom}&\textit{govoriti}&\textit{allt}\\
									
 \bottomrule
 \end{tabular}} \caption{Exemplification of the extracted translations 
 of English \textit{it} used as input features features in the 
 classificaton experiments.}\label{fig:featuresexample} 
 \end{figure*} 
 \end{center}


A manual analysis of a sample of 600 instances confirms that an important drawback of  
this method is the large number of examples for which a label cannot be determined, 
as shown in the column `Unknown' in Table \ref{tab:manualsample600} (these examples are not counted in our 22,615 labeled examples reported above). 
As for the examples that are labeled, the main 
problem is the annotation of pleonastics as nominals. Since we take pleonastic 
from the parsing annotation, these are therefore expletive constructions undetected by the parser that get labeled as nominals by our assumption that French \textit{il} corresponds to an `entity' reading. In addition, 
there is a natural imbalance in the classes, with 
nominal and pleonastic instances being largely more frequent than events.  

Concerning the quality of the annotation, it can
be seen in Table \ref{tab:manualsample600} that the automatic labeling achieves 
approximately 30\% accuracy overall (133/600) and 70\% accuracy if only successfully labeled  
examples are considered (133/189). A closer 
inspection of the `unknown' labels reveals that these are mostly due to many 
translations divergent from the assumptions we made by using French as the seed 
language. Another reason is alignment errors. 


\begin{table}[h!]\centering 
\begin{tabular}{p{1.3cm}|cccccc} 
&\multicolumn{4}{l}{\textbf{Automatic label $\rightarrow$}}\\
\toprule 
{\textbf{Gold $\downarrow$}}&Entity & Event &Pleonastic & Unknown \\ 
\midrule Entity &   56  &  5   &   0     &  259  \\ 
Event &    5  &  6   &  0  &  23\\ 
Pleonastic& 45 & 1&  71&  129\\
\bottomrule 
\end{tabular} \caption{Manual evaluation of a sample of 600
instances.}\label{tab:manualsample600} 
\end{table}

%\begin{table} \begin{tabular}{cccc} \multicolumn{4}{c}{\textbf{MaxEnt with
%oversampling}}\\ \toprule
% & Precision & Recall & Accuracy \\
%\midrule Entity & 0.73 &0.68&    0.80 \\ Event &0.91 & 1.0 & (8,277/10,347) \\
%Pleonastic &  0.74 & 0.72 & \\ \bottomrule \end{tabular}
%\caption{Classification results using bootstrap resampling in a manually
%annotated sample of 600 instances.} \end{table}
%
%
%
%
\section{Classification Experiments}\label{sec:experiments}

We used the 22,615 generated examples in a classification setting. All the 
experiments were completed using the implementations of the 
\texttt{scikit-learn} library, including their \texttt{train\_test\_split} 
function. 

In a first experiment, we use the extracted translations with the split in Table 
\ref{tab:split} to predict one of the three automatically generated labels: 
`entity', `event' or `pleonastic'. We report results using a Maximum Entropy 
classifier, although replication experiments using a SVM and a Naive Bayes 
classifier yielded very similar results. 


\begin{table}[h!]\centering
\begin{tabular}{ccc}
\toprule
\textbf{Train} & \textbf{Test} & \textbf{Total} \\
\midrule
15,887 & 6,728 & 22,615 \\
\bottomrule
\end{tabular}
\caption{Data set split for the classification experiments.}\label{tab:split}
\end{table}

Although the results using the automatic labels seem reasonable (Table  
\ref{tab:maxentautomatic}), when applying the same model to predict the manually 
annotated sample of 600 instances, we see a dramatic decrease in performance, in 
particular for the `event' class. As mentioned before, this class has a naturally 
low frequency, which makes it more 
difficult to predict in itself, with only 6 examples accurately labeled in the manual sample. 

\begin{center} \begin{table}[h!]\centering 
\begin{tabular}{l ccc}
\multicolumn{4}{c}{ \textbf{Automatically annotated data}}\\ 
\toprule
\textbf{MaxEnt}& Precision & Recall & Accuracy \\ 
\midrule 
Entity &0.70 & 0.75 &   0.70\\
Event & 0.44 & 0.15 & (4,710/6,728) \\
Pleonastic & 0.70 & 0.68&   \\ 
\midrule & & & \\
\multicolumn{4}{c}{\textbf{Manually annotated sample}}  \\ 
\midrule
\textbf{MaxEnt}& Precision & Recall & Accuracy \\ 
\midrule 
Entity &0.55 & 0.84 &0.54\\ 
Event &0.0 & 0.0 & (318/600)\\ 
Pleonastic & 0.50 & 0.22 & \\ 
\bottomrule
\end{tabular} \caption{Classification results using a Maximum Entropy
classifier.} \label{tab:maxentautomatic}
\end{table}
\end{center}

%At this point is important to have in mind that only 30\% of the examples in the 
%sample are actually automatically labeled, in the case of the `event' class in particular only 6 examples are accurately labeled. 

In order to determine whether the imbalance in the data is a factor preventing the model 
from learning the distinction, we used bootstrap with resampling in a second experiment so as to achieve the same number of examples per class. The data distribution for this experiment is given in Table \ref{tab:resamplingdata}.

\begin{table}[h!]\centering
\begin{tabular}{ccc}
\toprule
\textbf{Event} & \textbf{Entity} & \textbf{Pleonastic} \\
\midrule
11,377 & 11,377 & 11,377 \\
\bottomrule
\end{tabular}
\caption{Equal distribution of the classes for the experiment with 
oversampling.}\label{tab:resamplingdata}
\end{table}

In this second scenario, we obtained a comparable performance for the `entity' and `pleonastic' classes, and almost perfect scores for the `event' class (Table \ref{tab:maxentoversampling}). 

\begin{center} \begin{table}[h!] \begin{tabular}{lccc}
\multicolumn{4}{c}{\textbf{Oversampling of the event class}}\\ 
\toprule
\textbf{MaxEnt}& Precision & Recall & Accuracy \\ 
\midrule 
 Entity & 0.73 &0.67& 0.80 \\ 
 Event & 0.92 & 0.99 & (8,277/10,347) \\ 
  Pleonastic &  0.73 & 0.74 & \\
\bottomrule 
\end{tabular} 
\caption{Classification results using bootstrap with 
resampling to achieve an even distribution of the classes.}\label{tab:maxentoversampling}
\end{table}
\end{center}





\section{Discussion and Conclusion}

The experiments presented in the previous section suggest that relying on 
translations as features for the different readings of \textit{it} is a good 
method for 
the cases of \textit{it} that are captured by the seed language assumptions, 
most probably because  
these cases also provide a pronoun translation in the other languages. These 
represent about 30\% of the total amount of \textit{it}-pronouns, and unfortunately, they do not seem to generalize to the rest of the 
cases. 

Further analysis from the output of a decision tree classifier on the same data 
partition confirms this finding. As shown in Figure \ref{fig:decisiontree}, the 
top leaves in the tree all contain equivalent translations of either \textit{it} or 
\textit{this}, pronouns associated with `entity' and `event' respectively. 


\begin{figure}\small
\begin{verbatim}
|--- see_et <= 0.50
| |--- tas_lv <= 0.50
| | |--- este_ro <= 0.50
| | | |--- to_pl <= 0.50
| | | | |--- ez_hu <= 0.50
| | | | | |--- é_pt <= 0.50
| | | | | | |--- dies_de <= 0.50
| | | | | | | |--- je_sk <= 0.50
| | | | | | | | |--- se_fi <= 0.50
| | | | | | | | | |--- es_es <= 0.50
\end{verbatim}
\caption{Output of a decision tree classifier. The leaves shown correspond to
the top of the tree and have the form \texttt{pronoun\_language}.}\label{fig:decisiontree}
\end{figure}

% -- see_et<=0.5
%|  |--- é_pt<=0.5
%|  |  |--- tas_lv<=0.5
%|  |  |  |--- to_pl<=0.5
%|  |  |  |  |--- este_ro<=0.5
%|  |  |  |  |  |--- ez_hu<=0.5
%|  |  |  |  |  |  |--- es_es<=0.5
%|  |  |  |  |  |  |  |--- den_sv<=0.5
%|  |  |  |  |  |  |  |  |--- je_sk<=0.5
%|  |  |  |  |  |  |  |  |  |--- to_sk<=0.5
%|  |  |  |  |  |  |  |  |  |  |--se_fi<=0.5



Although we originally sought to identify systematic translation patterns 
indicative of non-nominal uses of \textit{it}, through 
developing this method, we found that apart from the pronoun-to-pronoun 
translation pattern, there is too much variability in the data. 

Take for instance the following examples:


\begin{enumerate}

\item[4.]\label{ex:freetrans} 
%Can we accept that there should be differences between Member States? Between 
%cities, in standards of protection for the urban environment? In road safety 
%standards in cities? In standards of citizens' access to mobility? Or are they 
%not fundamental rights that the Union should help to guarantee for all 
%Europeans?
\textsc{English} Well then, we need to establish standards and uniform minimum 
objectives, but also best practices and financial incentives. We need 
coordination and innovative projects, and to develop and share reliable and 
comparable statistics. If the Union takes \textbf{it} on, will \textbf{this} not help in 
realising those subsidiary solutions that Member States and local communities 
have every right to be protective of? 

\textsc{French} \textit{À cette fin, nous devons élaborer de les normes et de 
les 
objectifs minimaux communs, de bonnes pratiques et autres incitants financiers. 
Il faut une coordination; nous avons besoin de projets novateurs; nous devons 
travailler sur de les statistiques fiables et comparables qu'il faut pouvoir 
partager. Si l'Union \textbf{accepte}, ne \textbf{pourrons-nous} pas mettre en oeuvre ces 
solutions reposant sur la subsidiarité que les étâts membres et les communautés 
locales sont tout à fait en droit de protéger?}


\item[5.]\label{ex:shellnoun}\textsc{English} Madam President, Commissioners, 
can I 
say to you that less than 
a year ago we were debating in this Chamber what we were going to do about 
global food security, and was there enough food in the world, and we were 
terribly worried about \textbf{it}.

\textsc{French} \textit{Madame la Présidente, Mesdames et Messieurs les 
Commissaires, permettez-moi de vous rappeler qu'il y a moins d'un an, nous 
débattions en cette Assemblée de la manière de traiter la sécurité alimentaire 
mondiale, de la question de savoir si l'on produisait suffisamment de 
nourriture à l'échelle mondiale, et nous étions extrêmement préoccupés par 
\textbf{ces questions}}.

\end{enumerate}

In \ref{ex:freetrans}, the English \textit{it} disappears from the 
French translation as the choice in French is a complete reformulation along the 
lines of \textit{If the Union accepts, could not we implement\ldots}. In 
English, however, the \textit{it} must be taken cataphorically with the 
\textit{this} referring to the need \textit{to establish standards\ldots} and 
the exemplification sentence that follows. 

In example \label{ex:shellnoun}, on the other hand, the English \textit{it} 
refers to all what has previously been mentioned in the long sentence, a typical 
`event' reading of the pronoun. The French translation, however, prefers a 
translation with a full lexical noun phrase \textit{ces questions} (these 
questions) for the same referential 
relationship. This is a particular case of a shell-noun 
\cite{kolhatkar-etal-2013-interpreting}, and we believe that our method might be useful in identifying this phenomenon using multilingual parallel data. 

The task could also be approached semantically by identifying all abstract
nouns referencing actions, nominalizations, or eventualities in the text. Alternatively, one could focus on particular syntactic configurations as
\newcite{marasovic-etal-2017-mention}.

Non-nominal co-reference is much less studied than nominal coreference, partly
because of the lack of annotated corpora. In this paper, we have explored the
possibility of exploiting parallel multilingual corpora as a means of cheap
supervision for the task of \textit{it}-disambiguation. Since pronoun \textit{it} has many
potential uses or readings, we took it as representative of the non-nominal
coreference phenomenon; however, we found that only a very specific subset of examples are discernible using our approach. 

%\cite{Martin-90}
%
%\newcite{Martin-90}
%
%
\section{Acknowledgements}
Sharid Lo\'{a}iciga and Asad Sayeed were supported by the Swedish Research
Council under grant 2014-30 for the establishment of the
Centre for Linguistic Theory and Studies in Probability (CLASP) at
the University of Gothenburg.
Christian Hardmeier was supported by the Swedish Research Council under grant 2017-930.

%
%Place all acknowledgements (including those concerning research grants and
%funding) in a separate section at the end of the paper.
%
\section{References}

\bibliographystyle{lrec} \bibliography{lrec2020_it_manylanguages}


%\bibliographystylelanguageresource{lrec}
%\bibliographylanguageresource{languageresource}

\end{document}
